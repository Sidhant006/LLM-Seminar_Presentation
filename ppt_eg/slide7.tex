\begin{tikzpicture}[remember picture,overlay,xshift=8.8cm,yshift=-1.2cm]

% ===== HEADING =====
\node[anchor=west, text=offwhite, font=\bfseries\Large] 
at (-9.2,4.0) {Types of LLM Architectures};

% ===== EXPLANATION BLOCK =====
\node[anchor=west, text width=12cm, align=left, text=white, font=\small] 
at (-9.2,0.5) {

\textbf{1. Encoder-Only Models:}  
Used for understanding and classification tasks (e.g., BERT).\\[10pt]

\textbf{2. Decoder-Only Models:}  
Designed for text generation (e.g., GPT family).\\[10pt]

\textbf{3. Encoder--Decoder Models:}  
Effective for translation and summarization (e.g., T5).\\[10pt]

\textbf{4. Multimodal Models:}  
Handle text + images or audio (e.g., GPT-4V).\\[10pt]

\textbf{5. Mixture-of-Experts (MoE):}  
Routes input to specialized experts for efficiency.\\
};

\end{tikzpicture}
